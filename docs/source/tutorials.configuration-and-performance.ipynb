{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c206aaa-e894-4a55-8250-912317d0c52f",
   "metadata": {},
   "source": [
    "# CytoTable configuration and performance tutorial\n",
    "\n",
    "CytoTable performance may vary depending on the size and location of source data, available system resources, and how CytoTable is configured.\n",
    "This tutorial will provide light guidance on how to configure CytoTable based on your system resources.\n",
    "We focus this tutorial on two main configuration details which we observe as having the largest impact: __data chunk sizes__ and __Parsl configuration__.\n",
    "\n",
    "- __Data chunk sizes__: CytoTable uses the `chunk_size` parameter to create row-wise \"chunks\" of data operations which limits the total amount of memory used by procedures ([see here for more information](overview.md#data-chunking)).\n",
    "Larger chunk sizes can sometimes lead to faster time performance and larger memory footprints.\n",
    "Smaller chunk sizes can lead to slower time performance and smaller memory footprints.\n",
    "- __Parsl Configuration__: CytoTable uses [Parsl](https://parsl.readthedocs.io/en/stable/index.html) to efficiently process data through multi-step partially concurrent workflows and optional parallelism.\n",
    "Parsl provides a number of different configuration options which may be specified to CytoTable through the `parsl_config` parameter.\n",
    "\n",
    "The following is an example of how these configuration options are specified when using CytoTable:\n",
    "\n",
    "```python\n",
    "from parsl.config import Config\n",
    "from parsl.executors import ThreadPoolExecutor\n",
    "\n",
    "convert(\n",
    "        source_path=\"source_data_path\",\n",
    "        dest_path=\"destination_data_path\",\n",
    "        dest_datatype=\"parquet\",\n",
    "        # Here we set the data chunk size to be 10,000.\n",
    "        chunk_size=10000,\n",
    "        # Here we use Config and ThreadPoolExecutor \n",
    "        # objects to configure Parsl for threaded \n",
    "        # execution with defaults.\n",
    "        parsl_config=Config(\n",
    "            executors=[\n",
    "                ThreadPoolExecutor()\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "For more information, Parsl also provides [in-depth documentation on configuration](https://parsl.readthedocs.io/en/stable/userguide/index.html).\n",
    "\n",
    "## Additional performance considerations\n",
    "\n",
    "In addition to Parsl, a number of other elements may impact the performance you find with CytoTable.\n",
    "These are presented in no particular order in terms of impact (this will depend largely on the data sources and system resources available which have a wide variation).\n",
    "\n",
    "- __Cloud-based vs local data sources__: CytoTable provides source data capabilities through [cloudpathlib](https://cloudpathlib.drivendata.org/stable/), a package for interacting with cloud data storage through a unified API.\n",
    "Cloud-based data sources will generally be processed more slowly than locally available data in CytoTable.\n",
    "- __PyArrow settings__: CytoTable makes use of [PyArrow](https://arrow.apache.org/docs/python/index.html) for core in-memory data work for performance and integrative capabilities.\n",
    "    - PyArrow provides the ability to use non-default memory allocation which can sometimes enable greater performance.\n",
    "    CytoTable uses the default memory allocation selection performed by PyArrow. One may use the [`ARROW_DEFAULT_MEMORY_POOL`](https://arrow.apache.org/docs/cpp/env_vars.html#envvar-ARROW_DEFAULT_MEMORY_POOL) environment variable to specify which memory allocator is used by CytoTable (`jemalloc`, `mimalloc`, or `(C)malloc`)([see here for more](architecture.technical.md#arrow-memory-allocator-selection)).\n",
    "    - PyArrow operations through CytoTable use memory mapping (for example [`parquet.read_table(memory_mapped=...)`](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)) and may be turned off through the `CYTOTABLE_ARROW_USE_MEMORY_MAPPING` environment variable ([see here for more](architecture.technical.md#arrow-memory-mapping-selection)).\n",
    "- __DuckDB settings__: CytoTable uses [DuckDB](https://duckdb.org/docs/) to perform SQL-based data processing. DuckDB provides internal concurrency through the use of threads ([see here for more](https://duckdb.org/docs/connect/concurrency.html)).\n",
    "The number of threads defaults to the number of processors available on the system.\n",
    "Within CytoTable, the number of threads used by DuckDB may be explicitly set through the `CYTOTABLE_MAX_THREADS` environment variable.\n",
    "Please note: DuckDB is not used to write data in parallel through CytoTable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf8130-0848-424b-a506-f8e3a24a9a4e",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "We provide the following definitions to help clarify this content.\n",
    "\n",
    "### Concurrency and parallelism\n",
    "- [Concurrency](https://en.wikipedia.org/wiki/Concurrency_(computer_science)): the structure of computer program which allows for non-sequential execution without affecting the outcome.\n",
    "- [Parallelism](https://en.wikipedia.org/wiki/Parallel_computing): a type of computation in which more than one calculation may take place at the same time.\n",
    "\n",
    "### Processors and threads\n",
    "\n",
    "- [Processor](https://en.wikipedia.org/wiki/Central_processing_unit): a computer resource used to execute instructions from computer software. Computers may have one or many processors.\n",
    "- [Thread](https://en.wikipedia.org/wiki/Thread_(computing)): a sequence of computer software instructions executed by a processor. A processor may have one or many threads. A processor will only make progress one thread at a time.\n",
    "- [Multiprocessing](https://en.wikipedia.org/wiki/Multiprocessing): the use of more than one processor to accomplish a software task.\n",
    "- [Multithreading](https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)): the use of more than one thread to accomplish a software task.\n",
    "\n",
    "### Parsl\n",
    "\n",
    "- [Parsl Executors](https://parsl.readthedocs.io/en/stable/stubs/parsl.executors.base.ParslExecutor.html): abstractions which represent computer resources available to accomplish tasks through Parsl.\n",
    "- [`parsl.executors.ThreadPoolExecutor`](https://parsl.readthedocs.io/en/stable/stubs/parsl.executors.ThreadPoolExecutor.html): a  Parsl executor with multithreading capabilities.\n",
    "- [`parsl.executors.HighThroughputExecutor`](https://parsl.readthedocs.io/en/stable/stubs/parsl.executors.HighThroughputExecutor.html): a Parsl executor with multiprocessing capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6177d-e31d-40ce-9bdb-62c64bf38ff5",
   "metadata": {},
   "source": [
    "### Configuration performance heuristics\n",
    "\n",
    "Decisions about CytoTable configuration may benefit from understanding common heuristics about computing and technologies implemented by CytoTable.\n",
    "___Caveat emptor___: these are general guidance and may not be perfectly aligned to every system.\n",
    "\n",
    "- __Benchmark large data work on \"not too small\" data subsets__: it can take a few attempts to get the correct configuration for performance optimization.\n",
    "Save time by using a subset of your data to more quickly iterate through benchmarks of time and resource consumption.\n",
    "At the same time, consider using a subset which is not \"too\" small to help demonstrate realistic performance findings.\n",
    "- __Chunk sizes based on Parquet dataset file sizes:__ one way to estimate chunk size for a given source dataset is to use Parquet dataset file size.\n",
    "Parquet files are often thought to have the best performance when their storage size is around `100 MB` - `1024 MB` (`1 GB`).\n",
    "- __Best number of threads__: the number of threads used by software is typically set to the number of processors available (sometimes multiplied by small integer, as in the case of Python's [`ThreadPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor).).\n",
    "- __Avoid too much multitasking (multithread or multiprocess tasks)__: there are limits to the benefits received through multitasking.\n",
    "Each task has an inherent resource cost overhead for management in addition to the work it will accomplish.\n",
    "Be sure to consider reasonable numbers of multithreaded or multiprocessed tasks to avoid too much management overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245be86b-7e5b-4e89-96f0-cd3e3949c04e",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "\n",
    "This tutorial will demonstrate the information above using code below.\n",
    "\n",
    "#### Demonstration environment\n",
    "\n",
    "This demonstration makes use of development dependencies of CytoTable.\n",
    "We recommend cloning the repository and using the [Contributing getting started](contributing.md#getting-started) documentation for help recreating the environment used here.\n",
    "\n",
    "#### Demonstration dataset\n",
    "\n",
    "We leverage a \"not too small\" source data file from the [Cell Painting Gallery](https://github.com/broadinstitute/cellpainting-gallery) `cpg0016-jump` dataset ([preprint here](https://doi.org/10.1101/2023.03.23.534023)) to help demonstrate how configuration impacts performance within CytoTable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44411896-b615-411d-af2f-ade4c22c56f0",
   "metadata": {},
   "source": [
    "### How can we pick a \"not too small\" dataset?\n",
    "\n",
    "We can use [file globbing](https://en.wikipedia.org/wiki/Glob_(programming)) through pathnames to determine a file which is small but not too small for use with performance benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de55e717-1763-4f6c-8f92-a579ee2ffe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BR00126114/BR00126114.sqlite', '04 GB'),\n",
       " ('BR00126115/BR00126115.sqlite', '10 GB'),\n",
       " ('BR00126712/BR00126712.sqlite', '17 GB'),\n",
       " ('BR00126116/BR00126116.sqlite', '18 GB'),\n",
       " ('BR00126708/BR00126708.sqlite', '18 GB'),\n",
       " ('BR00126709/BR00126709.sqlite', '18 GB'),\n",
       " ('BR00126711/BR00126711.sqlite', '18 GB'),\n",
       " ('BR00126716/BR00126716.sqlite', '18 GB'),\n",
       " ('BR00126717/BR00126717.sqlite', '18 GB'),\n",
       " ('BR00126718/BR00126718.sqlite', '18 GB'),\n",
       " ('BR00126706/BR00126706.sqlite', '19 GB'),\n",
       " ('BR00126710/BR00126710.sqlite', '19 GB'),\n",
       " ('BR00126714/BR00126714.sqlite', '19 GB'),\n",
       " ('BR00126715/BR00126715.sqlite', '20 GB'),\n",
       " ('BR00126113/BR00126113.sqlite', '22 GB'),\n",
       " ('BR00126707/BR00126707.sqlite', '26 GB'),\n",
       " ('BR00126117/BR00126117.sqlite', '28 GB')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "from cloudpathlib import S3Client\n",
    "\n",
    "# set a path based on the above\n",
    "cloud_source_data_path = (\n",
    "    \"s3://cellpainting-gallery/cpg0016-jump/\"\n",
    "    \"source_4/workspace/backend/2021_08_23_Batch12/\"\n",
    ")\n",
    "\n",
    "s3_client = S3Client(no_sign_request=True)\n",
    "\n",
    "# show sorted results for recursively globbed sqlite filepaths based on filesize\n",
    "sorted(\n",
    "    {\n",
    "        f\"{path.parent.name}/{path.name}\": f\"{round(path.stat().st_size / 1024 / 1024 / 1024):02} GB\"\n",
    "        for path in s3_client.CloudPath(\n",
    "            cloud_source_data_path,\n",
    "        ).rglob(\"*.sqlite\")\n",
    "    }.items(),\n",
    "    # sort by dictionary values\n",
    "    key=lambda item: item[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4a5dd29-5b40-4e6b-a485-06b796fc9c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('BR00126114.sqlite')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download a \"not too small\" dataset from the above information.\n",
    "local_file = pathlib.Path(\"./BR00126114.sqlite\")\n",
    "\n",
    "# check if we already have the file, if not download it\n",
    "if not local_file.is_file():\n",
    "    s3_client.CloudPath(\n",
    "        cloud_source_data_path + \"BR00126114/BR00126114.sqlite\",\n",
    "    ).download_to(destination=\".\")\n",
    "\n",
    "# show the local file\n",
    "local_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c649330-a023-44c9-a6b1-2244eff599b9",
   "metadata": {},
   "source": [
    "### How can we determine what chunk size to start with?\n",
    "\n",
    "We can use knowledge about the dataset table row length and export to file using varying chunk sizes to find a file size between 100 MB - 1 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2837bda-9373-45c9-b0be-611145e6afde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image': 3345, 'Cells': 74226, 'Cytoplasm': 74226, 'Nuclei': 74226}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from contextlib import closing\n",
    "\n",
    "table_row_counts = {}\n",
    "\n",
    "# gather table names from the data\n",
    "with closing(sqlite3.connect(str(local_file))) as cx:\n",
    "    # We build a contextlib.closing context to close the database\n",
    "    # connection automatically instead of closing explicitly.\n",
    "    with cx:\n",
    "        cursor = cx.execute(\n",
    "            \"\"\"\n",
    "            /* we use a special SQLite reference here\n",
    "            called `sqlite_master` to attain metadata\n",
    "            about the database */\n",
    "            SELECT tbl_name\n",
    "            FROM sqlite_master \n",
    "            WHERE type='table';\n",
    "            \"\"\"\n",
    "        )\n",
    "        table_names = [elem[0] for elem in cursor.fetchall()]\n",
    "\n",
    "        # loop using each table name\n",
    "        for table_name in table_names:\n",
    "            cursor = cx.execute(\n",
    "                f\"\"\"\n",
    "                SELECT COUNT(*)\n",
    "                FROM {table_name}\n",
    "                \"\"\"\n",
    "            )\n",
    "            table_row_counts[table_name] = cursor.fetchone()[0]\n",
    "\n",
    "# show the table names and row counts\n",
    "table_row_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1aeff6d3-dc10-4981-b784-d4a050dcc7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file size with chunk size of 10000: 147 MB\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "chunk_size_to_try = 10000\n",
    "benchmark_filesize_file = \"example-benchmark.parquet\"\n",
    "\n",
    "# use duckdb to extract a \"chunk\" of data using chunk_size\n",
    "# and exporting to Parquet file.\n",
    "with duckdb.connect() as ddb:\n",
    "    ddb.execute(\n",
    "        \"\"\"\n",
    "        /* Install and load sqlite plugin for duckdb */\n",
    "        INSTALL sqlite_scanner;\n",
    "        LOAD sqlite_scanner;\n",
    "        \"\"\"\n",
    "    )\n",
    "    ddb.execute(\n",
    "        f\"\"\"\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            /* duckdb allows us to use a special function to\n",
    "            access SQLite database tables directly as seen here */\n",
    "            FROM sqlite_scan({str(local_file)}, 'Cells')\n",
    "            LIMIT {chunk_size_to_try}\n",
    "        )\n",
    "        /* here we export to a file of parquet format type */\n",
    "        TO '{benchmark_filesize_file}'\n",
    "        (FORMAT PARQUET)\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Parquet file size with chunk size of {chunk_size_to_try}:\",\n",
    "    round(pathlib.Path(benchmark_filesize_file).stat().st_size / 1024 / 1024),\n",
    "    \"MB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bcee5a-7768-4857-ab88-356f6904f21c",
   "metadata": {},
   "source": [
    "### How can we estimate the number of threads to use?\n",
    "\n",
    "We can use the number of processors on the system as a rough estimate for number of threads to use.\n",
    "Recall that this is only an heuristic as the number of possible threads to use is much higher but may come with an imbalance from thread management overhead (or other aspects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e7ec590-462f-4c7c-b420-c331110b7c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "number_of_threads_to_try = multiprocessing.cpu_count()\n",
    "print(number_of_threads_to_try)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae841669-9d68-475f-beb5-ef347f299169",
   "metadata": {},
   "source": [
    "### How does multithreaded performance change with different configurations?\n",
    "\n",
    "We can use [`parsl.executors.ThreadPoolExecutor`](https://parsl.readthedocs.io/en/stable/stubs/parsl.executors.ThreadPoolExecutor.html) to test various numbers of threads and other configuration to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2b826-c40e-45bc-bca5-44d11838cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing previously loaded Parsl configuration.\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "\n",
    "from parsl.config import Config\n",
    "from parsl.executors import ThreadPoolExecutor\n",
    "\n",
    "import cytotable\n",
    "\n",
    "result = cytotable.convert(\n",
    "    source_path=str(local_file),\n",
    "    dest_path=f\"{str(local_file.name)}.parquet\",\n",
    "    dest_datatype=\"parquet\",\n",
    "    preset=\"cell-health-cellprofiler-to-cytominer-database\",\n",
    "    chunk_size=chunk_size_to_try,\n",
    "    parsl_config=Config(\n",
    "        executors=[ThreadPoolExecutor(max_threads=number_of_threads_to_try)]\n",
    "    ),\n",
    "    joins=\"\"\"WITH Image_Filtered AS (\n",
    "                SELECT\n",
    "                    Metadata_TableNumber,\n",
    "                    Metadata_ImageNumber,\n",
    "                    Metadata_Well,\n",
    "                    Image_Metadata_Plate\n",
    "                FROM\n",
    "                    read_parquet('image.parquet')\n",
    "                )\n",
    "            SELECT\n",
    "                *\n",
    "            FROM\n",
    "                Image_Filtered AS image\n",
    "            LEFT JOIN read_parquet('cytoplasm.parquet') AS cytoplasm ON\n",
    "                cytoplasm.Metadata_TableNumber = image.Metadata_TableNumber\n",
    "                AND cytoplasm.Metadata_ImageNumber = image.Metadata_ImageNumber\n",
    "            LEFT JOIN read_parquet('cells.parquet') AS cells ON\n",
    "                cells.Metadata_TableNumber = cytoplasm.Metadata_TableNumber\n",
    "                AND cells.Metadata_ImageNumber = cytoplasm.Metadata_ImageNumber\n",
    "                AND cells.Cells_ObjectNumber = cytoplasm.Metadata_Cytoplasm_Parent_Cells\n",
    "            LEFT JOIN read_parquet('nuclei.parquet') AS nuclei ON\n",
    "                nuclei.Metadata_TableNumber = cytoplasm.Metadata_TableNumber\n",
    "                AND nuclei.Metadata_ImageNumber = cytoplasm.Metadata_ImageNumber\n",
    "                AND nuclei.Nuclei_ObjectNumber = cytoplasm.Metadata_Cytoplasm_Parent_Nuclei\n",
    "\n",
    "        \"\"\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75cffe2d-0e7a-4a46-85d0-d6835b02caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the result\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"BR00126114.sqlite.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2eddb-9836-4708-a9d7-54c00b4887a4",
   "metadata": {},
   "source": [
    "### How does multprocessed performance change with different configurations?\n",
    "\n",
    "We can use [`parsl.executors.HighThroughputExecutor`](https://parsl.readthedocs.io/en/stable/stubs/parsl.executors.HighThroughputExecutor.html) to test various numbers of blocks and other configuration to see what happens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
